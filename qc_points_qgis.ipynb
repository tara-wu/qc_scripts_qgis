{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e580909-bb3f-4f8c-b50a-3620d5318b61",
   "metadata": {},
   "source": [
    "## Quality Control Script for ArcGIS Online Points Layer\n",
    "by Tara Wu, Spring 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbbb0ec-7656-4c25-ac89-3be274883dc2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Purpose:</b>  This script performs a series of quality checks for the Legacy Restoration Fund project in New England. As damage points along the Appalachian Trail are being collected, the resulting dataset will be checked for the following: \n",
    "<ul>\n",
    "    <li>nulls in specific fields invalid dates orphaned related records</li>\n",
    "    <li>missing related records </li>\n",
    "    <li> duplicates due to sync error,</li> \n",
    "    <li>inputs not in domains, repetitive attributes per user </li>\n",
    "    <li>offline data has been synced</li>\n",
    "    <li>proximity to trails/features</li> \n",
    "    <li>matching collector and region</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb592dcd-723d-4bd1-b40d-b7bdc331544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PARAMETERS ===\n",
    "\n",
    "# === import modules ===\n",
    "import datetime\n",
    "import importlib\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from arcgis.features import FeatureLayer\n",
    "from arcgis.gis import GIS\n",
    "from shapely.geometry import shape\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "# check for required packages\n",
    "required_packages = [\"pandas\", \"geopandas\", \"numpy\", \"arcgis\", \"shapely\", \"xlsxwriter\"]\n",
    "\n",
    "for pkg in required_packages:\n",
    "    if importlib.util.find_spec(pkg) is None:\n",
    "        print(f\"Missing package '{pkg}'. Install via: pip install {pkg}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "# === output setup ===\n",
    "# base repo directory\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# output folder and files\n",
    "OUTPUT_FOLDER = os.path.join(BASE_DIR, \"outputs\")  # outputs folder inside repo\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# current date for outputs\n",
    "CURRENT_DATE = datetime.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# output files\n",
    "FILE_PATH = os.path.join(OUTPUT_FOLDER, f\"{CURRENT_DATE}_QC_summary.txt\")\n",
    "OUTPUT_XLSX = os.path.join(OUTPUT_FOLDER, f\"{CURRENT_DATE}_all_errors.xlsx\")\n",
    "\n",
    "\n",
    "# === connect to AGOL services ===\n",
    "# choose login method: \"pro\", \"home\", or \"credentials\"\n",
    "GIS_LOGIN_METHOD = \"pro\"  # change to \"home\" if running in AGOL Notebooks\n",
    "\n",
    "# if using credentials (not recommended for shared scripts), fill in here:\n",
    "GIS_USERNAME = None\n",
    "GIS_PASSWORD = None\n",
    "\n",
    "# connect to AGOL\n",
    "try:\n",
    "    if GIS_LOGIN_METHOD == \"pro\":\n",
    "        gis = GIS(\"pro\")\n",
    "    elif GIS_LOGIN_METHOD == \"home\":\n",
    "        gis = GIS(\"home\")\n",
    "    elif GIS_LOGIN_METHOD == \"credentials\":\n",
    "        gis = GIS(GIS_USERNAME, GIS_PASSWORD)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid GIS_LOGIN_METHOD. Use 'pro', 'home', or 'credentials'.\"\n",
    "        )\n",
    "\n",
    "except Exception as e:\n",
    "    logging.info(\"X   Login error\")\n",
    "    logging.info(f\"    {type(e).__name__}: {e}\\n\")\n",
    "else:\n",
    "    if gis.users.me:\n",
    "        logging.info(f\"    Login successful as {gis.users.me.username}\\n\")\n",
    "    else:\n",
    "        logging.info(\"X   Not logged in. Please sign in.\\n\")\n",
    "\n",
    "# base URLs for AGOL feature services\n",
    "BASE_URL = \"https://services1.arcgis.com/fBc8EJBxQRMcHlei/arcgis/rest/services/APPA_LRF_ProjectsV2/FeatureServer\"\n",
    "FACILITIES_URL = \"https://services1.arcgis.com/fBc8EJBxQRMcHlei/arcgis/rest/services/ANST_Facilities/FeatureServer\"\n",
    "\n",
    "# feature service URLs\n",
    "FC_URL = f\"{BASE_URL}/0\"  # LRF Tread Deficiency\n",
    "RT_URL = f\"{BASE_URL}/1\"  # Related Table\n",
    "TREAD_URL = f\"{FACILITIES_URL}/7\"\n",
    "SIDETRAIL_URL = f\"{FACILITIES_URL}/6\"\n",
    "POINT_FEATURE_URLS = [\n",
    "    f\"{FACILITIES_URL}/0\",  # bridges\n",
    "    f\"{FACILITIES_URL}/1\",  # campsites\n",
    "    f\"{FACILITIES_URL}/2\",  # parking\n",
    "    f\"{FACILITIES_URL}/3\",  # privies\n",
    "    f\"{FACILITIES_URL}/4\",  # shelters\n",
    "    f\"{FACILITIES_URL}/5\",  # vistas\n",
    "]\n",
    "\n",
    "\n",
    "# === QC parameters===\n",
    "BUFFER_FT = 100\n",
    "THRESHOLD = 0.9\n",
    "PROJECT_START_YEAR = 2025\n",
    "PROJECT_START_MONTH = 5\n",
    "PROJECT_START_DAY = 5\n",
    "\n",
    "# fields that shouldn't be null\n",
    "FC_REQUIRED_FIELDS = [\"GlobalID\", \"created_user\", \"created_date\", \"SHAPE\"]\n",
    "RT_REQUIRED_FIELDS = [\"defGlobalID\", \"Feature\", \"Feature_Action\"]\n",
    "\n",
    "# fields that suggest sync errors if containing duplicated data\n",
    "FC_SYNC_ERROR_FIELDS = [\"created_date\", \"created_user\"]\n",
    "RT_SYNC_ERROR_FIELDS = [\"CreationDate\", \"Creator\"]\n",
    "\n",
    "# domain dictionaries\n",
    "FC_DOMAIN_DICTIONARY = {\n",
    "    \"State\": [\"MA\", \"ME\", \"CT\", \"NH\", \"VT\"],\n",
    "    \"Club\": [\"AMC\", \"AMC-CT\", \"AMC-WMA\", \"DOC\", \"GMC\", \"MATC\", \"RMC\"],\n",
    "    \"Evaluation_Code\": [\"Low\", \"Moderate\", \"High\", None],\n",
    "    \"OnsiteMaterials\": [\"Yes\", \"No\", \"Maybe\", None],\n",
    "    \"ConsiderRelocation\": [\"Yes\", \"No\", None],\n",
    "}\n",
    "\n",
    "RT_DOMAIN_DICTIONARY = {\n",
    "    \"Feature\": [...],  # keep as in your script\n",
    "    \"FeatureAction\": [\"Build Add\", \"Repair Replace\", \"Remove\", None],\n",
    "    \"Units\": [\"Each\", \"LinearFeet\", \"SquareFeet\", None],\n",
    "    \"onsitematerials\": [\"Yes\", \"No\", \"Maybe\", None],\n",
    "}\n",
    "\n",
    "# fields to check for repetitive input\n",
    "FC_REP_ERROR_FIELDS = [\"Evaluation_Code\", \"Deficiency_Length\"]\n",
    "RT_REP_ERROR_FIELDS = [\"Feature\"]\n",
    "\n",
    "# collector-region dictionary\n",
    "COLLECTOR_DICT = {\n",
    "    \"twu_ATConservancy\": [\"MATC\"],\n",
    "    \"userB\": [\"RMC\", \"AMC\", \"DOC\"],\n",
    "    \"userC\": [\"GMC\"],\n",
    "    \"userD\": [\"AMC-WMA\"],\n",
    "    \"userE\": [\"AMC-CT\"],\n",
    "}\n",
    "\n",
    "\n",
    "# === output parameters ===\n",
    "\n",
    "WRITE_LOG = True  # set to False if you only want console output\n",
    "\n",
    "# column order for outputs\n",
    "FC_ERROR_ORDER = [\n",
    "    \"error_type\",\n",
    "    \"error_desc\",\n",
    "    \"OBJECTID\",\n",
    "    \"GlobalID\",\n",
    "    \"created_user\",\n",
    "    \"created_date\",\n",
    "    \"Deficiency_Length\",\n",
    "    \"Evaluation_Code\",\n",
    "    \"ConsiderRelocation\",\n",
    "    \"Notes\",\n",
    "    \"OnsiteMaterials\",\n",
    "    \"RelativeLinearLocation\",\n",
    "    \"SHAPE\",\n",
    "    \"MileMarker\",\n",
    "    \"State\",\n",
    "    \"LandOwner\",\n",
    "    \"OwnershipType\",\n",
    "    \"Club\",\n",
    "]\n",
    "\n",
    "RT_ERROR_ORDER = [\n",
    "    \"error_type\",\n",
    "    \"error_desc\",\n",
    "    \"OBJECTID\",\n",
    "    \"GlobalID\",\n",
    "    \"defGlobalID\",\n",
    "    \"CreationDate\",\n",
    "    \"Creator\",\n",
    "    \"Feature\",\n",
    "    \"Feature_Action\",\n",
    "    \"Quantity\",\n",
    "    \"Units\",\n",
    "    \"onsitematerials\",\n",
    "]\n",
    "\n",
    "\n",
    "# === CONNECT TO RESOURCES, LOAD SURVEY DATA ===\n",
    "\n",
    "# log start time to measure elapsed time for full code\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# configure console and txt file output\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "handlers = [logging.StreamHandler(sys.stdout)]\n",
    "if WRITE_LOG:\n",
    "    handlers.append(logging.FileHandler(FILE_PATH, mode=\"w\", encoding=\"utf-8\"))\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(message)s\",\n",
    "    handlers=handlers,\n",
    ")\n",
    "\n",
    "\n",
    "# connect to feature service\n",
    "# === CONNECT TO FEATURE SERVICES ===\n",
    "try:\n",
    "    fc_layer = FeatureLayer(FC_URL)\n",
    "    rt_layer = FeatureLayer(RT_URL)\n",
    "    tread_layer = FeatureLayer(TREAD_URL)\n",
    "    sidetrail_layer = FeatureLayer(SIDETRAIL_URL)\n",
    "    feature_layers = [FeatureLayer(url) for url in POINT_FEATURE_URLS]\n",
    "\n",
    "except Exception as e:\n",
    "    logging.info(\"X   Error loading feature service URLs\\n\")\n",
    "    logging.info(f\"    {type(e).__name__}: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\n",
    "        \"    URLs loaded for survey point data, related table, tread, side trail, and point features\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "# load survey and line data into sdf\n",
    "try:\n",
    "    fc_features = fc_layer.query(where=\"1=1\", out_fields=\"*\", return_geometry=True).sdf\n",
    "    rt_features = rt_layer.query(where=\"1=1\", out_fields=\"*\").sdf\n",
    "\n",
    "    tread_features = tread_layer.query(\n",
    "        where=\"1=1\", out_fields=\"*\", return_geometry=True\n",
    "    ).sdf\n",
    "    sidetrail_features = sidetrail_layer.query(\n",
    "        where=\"1=1\", out_fields=\"*\", return_geometry=True\n",
    "    ).sdf\n",
    "except:\n",
    "    logging.info(\"X   Error converting to spatially-enabled data frames\\n\")\n",
    "else:\n",
    "    logging.info(\n",
    "        \"    Converted survey point data, related table, tread, and side trail to spatially-enabled data frames\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "# === QUALITY CHECKS ===\n",
    "\n",
    "fc_error_rows = []\n",
    "rt_error_rows = []\n",
    "summary_rows = []\n",
    "\n",
    "\n",
    "# === null checks ===\n",
    "# function to check nulls, collect rows for error report\n",
    "def check_nulls(df, df_name, fields, error_list):\n",
    "    any_nulls = False\n",
    "    for field in fields:\n",
    "        if field in df.columns:\n",
    "            nulls = df[df[field].isnull()].copy()\n",
    "            if not nulls.empty:\n",
    "                any_nulls = True\n",
    "                nulls.loc[:, \"error_type\"] = \"nulls\"\n",
    "                nulls.loc[:, \"error_desc\"] = f\"NULL in {df_name} field: {field}\"\n",
    "                error_list.append(nulls)\n",
    "                message = f\"{df_name}.{field}: {len(nulls)} nulls\"\n",
    "                logging.info(\"        \" + message)\n",
    "                summary_rows.append(message)\n",
    "        else:\n",
    "            logging.info(f\"        {field} not in list of fields for {df_name}\")\n",
    "\n",
    "    if not any_nulls:\n",
    "        message = f\"0 null errors found in {df_name}\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "\n",
    "\n",
    "# run null checks\n",
    "try:\n",
    "    summary_rows.append(\"Null errors:\")\n",
    "    check_nulls(fc_features, \"fc\", FC_REQUIRED_FIELDS, fc_error_rows)\n",
    "    check_nulls(rt_features, \"rt\", RT_REQUIRED_FIELDS, rt_error_rows)\n",
    "    summary_rows.append(\"\")  # blank line between error checks\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error running null check\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\"    Null check completed\\n\")\n",
    "\n",
    "\n",
    "# === date check ===\n",
    "# check for dates in the future or prior to the beginning of the project\n",
    "try:\n",
    "    project_start_date = datetime.datetime(\n",
    "        PROJECT_START_YEAR, PROJECT_START_MONTH, PROJECT_START_DAY\n",
    "    )\n",
    "    today = datetime.datetime.today()\n",
    "\n",
    "    fc_features[\"created_date\"] = pd.to_datetime(\n",
    "        fc_features[\"created_date\"], errors=\"coerce\"\n",
    "    )\n",
    "    invalid_dates = fc_features[\n",
    "        (fc_features[\"created_date\"] > today)\n",
    "        | (fc_features[\"created_date\"] < project_start_date)\n",
    "    ].copy()\n",
    "\n",
    "    if not invalid_dates.empty:\n",
    "        invalid_dates.loc[:, \"error_type\"] = \"dates\"\n",
    "        invalid_dates.loc[:, \"error_desc\"] = \"Invalid observation date\"\n",
    "        fc_error_rows.append(invalid_dates)\n",
    "    message = f\"{len(invalid_dates)} records with invalid dates\"\n",
    "    logging.info(\"        \" + message)\n",
    "    summary_rows.append(\"Date errors:\")\n",
    "    summary_rows.append(message)\n",
    "    summary_rows.append(\"\")  # blank line between error checks\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error running date check\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "    logging.info(traceback.format_exc())\n",
    "else:\n",
    "    logging.info(\"    Date check completed\\n\")\n",
    "\n",
    "\n",
    "# === orphaned related records check ===\n",
    "# check for records in table that aren't related to a fc point\n",
    "try:\n",
    "    valid_ids = set(fc_features[\"GlobalID\"])\n",
    "    orphaned = rt_features[~rt_features[\"defGlobalID\"].isin(valid_ids)].copy()\n",
    "    orphaned.loc[:, \"error_type\"] = \"orphaned\"\n",
    "    orphaned.loc[:, \"error_desc\"] = \"Prescription record has no related tread feature\"\n",
    "    rt_error_rows.append(orphaned)\n",
    "    message = f\"{len(orphaned)} orphaned prescription records\"\n",
    "    logging.info(\"        \" + message)\n",
    "    summary_rows.append(\"Orphaned record errors:\")\n",
    "    summary_rows.append(message)\n",
    "    summary_rows.append(\"\")  # blank line between error checks\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error running orphaned records check\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\"    Orphaned related records check completed\\n\")\n",
    "\n",
    "\n",
    "# === missing related records check ===\n",
    "# check for fc points that aren't related to any records in the related table\n",
    "try:\n",
    "    related_counts = rt_features[\"defGlobalID\"].value_counts()\n",
    "    fc_features[\"related_count\"] = (\n",
    "        fc_features[\"GlobalID\"].map(related_counts).fillna(0).astype(int)\n",
    "    )\n",
    "    missing_related = fc_features[fc_features[\"related_count\"] == 0].copy()\n",
    "    missing_related.loc[:, \"error_type\"] = \"missing related\"\n",
    "    missing_related.loc[:, \"error_desc\"] = (\n",
    "        \"Tread feature has no related prescription records\"\n",
    "    )\n",
    "    fc_error_rows.append(missing_related)\n",
    "    message = f\"{len(missing_related)} features with no related prescriptions\"\n",
    "    logging.info(\"        \" + message)\n",
    "    summary_rows.append(\"Related record errors:\")\n",
    "    summary_rows.append(message)\n",
    "    summary_rows.append(\"\")  # blank line between error checks\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error running related records check\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\"    Missing related records check completed\\n\")\n",
    "\n",
    "\n",
    "# === sync error checks ===\n",
    "# check for sync errors via duplicated date/time and user fields, collect rows for error report\n",
    "def check_sync_errors(df, df_name, fields, error_list):\n",
    "    any_sync_errors = False\n",
    "    dup_keys = df.groupby(fields).size().reset_index(name=\"count\")\n",
    "    dup_keys = dup_keys[dup_keys[\"count\"] > 1]\n",
    "\n",
    "    if not dup_keys.empty:\n",
    "        any_sync_errors = True\n",
    "        # Merge back to get full duplicate records\n",
    "        sync_errors = df.merge(dup_keys[fields], on=fields, how=\"inner\")\n",
    "        sync_errors.loc[:, \"error_type\"] = \"sync\"\n",
    "        sync_errors.loc[:, \"error_desc\"] = f\"Potential sync error in {df_name}\"\n",
    "        error_list.append(sync_errors)\n",
    "        message = f\"{len(sync_errors)} potential sync errors in {df_name}\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "    else:\n",
    "        logging.info(f\"        0 potential sync errors found in {df_name}\")\n",
    "\n",
    "    if not any_sync_errors:\n",
    "        summary_rows.append(f\"0 potential sync errors found in {df_name}\")\n",
    "\n",
    "\n",
    "# run sync error check\n",
    "try:\n",
    "    summary_rows.append(\"Potential sync errors:\")\n",
    "    check_sync_errors(fc_features, \"fc\", FC_SYNC_ERROR_FIELDS, fc_error_rows)\n",
    "    check_sync_errors(rt_features, \"rt\", RT_SYNC_ERROR_FIELDS, rt_error_rows)\n",
    "    summary_rows.append(\"\")  # blank line between error checks\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error running sync errors check\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\"    Sync error checks completed\\n\")\n",
    "\n",
    "\n",
    "# === domains check ===\n",
    "# function to check domains, collect rows for error report\n",
    "def check_domains(df, df_name, domain_dict, error_list):\n",
    "    any_domain_errors = False\n",
    "    for d in domain_dict:\n",
    "        if d in df.columns:\n",
    "            valid_values = domain_dict[d]\n",
    "            # Identify invalid rows (not in valid list and not null)\n",
    "            invalid_domain = df[~(df[d].isin(valid_values) | df[d].isnull())].copy()\n",
    "            if not invalid_domain.empty:\n",
    "                any_domain_errors = True\n",
    "                # Get unique invalid values for this column\n",
    "                unique_invalids = invalid_domain[d].dropna().unique()\n",
    "                invalid_domain.loc[:, \"error_type\"] = \"domains\"\n",
    "                invalid_domain.loc[:, \"error_desc\"] = invalid_domain[d].apply(\n",
    "                    lambda x: f\"'{x}' not in domains for {d}\"\n",
    "                )\n",
    "                error_list.append(invalid_domain)\n",
    "                message = f\"Invalid entries for field '{d}' in {df_name}: {list(unique_invalids)}\"\n",
    "                logging.info(\"        \" + message)\n",
    "                summary_rows.append(message)\n",
    "\n",
    "    if not any_domain_errors:\n",
    "        message = f\"0 domain errors found in {df_name}\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "\n",
    "\n",
    "# run domain check\n",
    "try:\n",
    "    summary_rows.append(\"Domain errors:\")\n",
    "    check_domains(fc_features, \"fc\", FC_DOMAIN_DICTIONARY, fc_error_rows)\n",
    "    check_domains(rt_features, \"rt\", RT_DOMAIN_DICTIONARY, rt_error_rows)\n",
    "    summary_rows.append(\"\")  # blank line between error checks\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error running domain check\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\"    Domain checks completed\\n\")\n",
    "\n",
    "\n",
    "# === repetitive attribute check ===\n",
    "# NOTE there is potential for this to flag false positives, e.g. if there just\n",
    "# happens to be a predominance of one feature type.  This will also\n",
    "# flag a false positive if a user only logs one point\n",
    "\n",
    "\n",
    "def check_repetitive_values(df, df_name, field_list, error_list, THRESHOLD):\n",
    "    # currently, there are different column names for fc and rt\n",
    "    if \"created_user\" in df.columns:\n",
    "        user_field = \"created_user\"\n",
    "    elif \"Creator\" in df.columns:\n",
    "        user_field = \"Creator\"\n",
    "    else:\n",
    "        logging.info(\"no username field found\")\n",
    "        return\n",
    "\n",
    "    # group by user\n",
    "    grouped = df.groupby(user_field)\n",
    "    rep_error_count = 0\n",
    "\n",
    "    # loop through each user's records\n",
    "    for user, group in grouped:\n",
    "        # check values in fields identified as having potential repetition\n",
    "        for field in field_list:\n",
    "            if field in group.columns:\n",
    "                # calculate frequency of value in field as proportion of user's total (how many times a particular user added x in a field / total number of records they logged)\n",
    "                value_counts = group[field].value_counts(normalize=True)\n",
    "\n",
    "                # filter for values with proportion above specified threshold\n",
    "                dominant_values = value_counts[value_counts > THRESHOLD]\n",
    "\n",
    "                # filter user's records for dominant values and append to error list\n",
    "                for val in dominant_values.index:\n",
    "                    rep_errors = group[group[field] == val].copy()\n",
    "                    rep_errors.loc[:, \"error_type\"] = \"repetitive\"\n",
    "                    rep_errors.loc[:, \"error_desc\"] = (\n",
    "                        f\"{user} repeated '{val}' in {df_name}.{field} over {int(THRESHOLD*100)}% of the time\"\n",
    "                    )\n",
    "                    error_list.append(rep_errors)\n",
    "                    rep_error_count += len(rep_errors)\n",
    "\n",
    "    if rep_error_count > 0:\n",
    "        message = f\"{rep_error_count} potentially repetitive inputs found in {df_name}\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "    else:\n",
    "        message = f\"0 potentially repetitive inputs found in {df_name}\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "\n",
    "\n",
    "# run repetitive value check\n",
    "try:\n",
    "    summary_rows.append(\"Repetitive value errors:\")\n",
    "    check_repetitive_values(\n",
    "        fc_features, \"fc\", FC_REP_ERROR_FIELDS, fc_error_rows, THRESHOLD\n",
    "    )\n",
    "    check_repetitive_values(\n",
    "        rt_features, \"rt\", RT_REP_ERROR_FIELDS, rt_error_rows, THRESHOLD\n",
    "    )\n",
    "    summary_rows.append(\"\")  # blank line between error checks\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error running repetitive values check\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\"    Repetitive values error checks completed\\n\")\n",
    "\n",
    "\n",
    "# === offline data sync check ===\n",
    "# NOTE this functionality is contingent on getting user logs from field staff.\n",
    "# then compare user log dates to AGOL dates\n",
    "\n",
    "\n",
    "# === proximity checks ===\n",
    "try:\n",
    "    # any_proximity_errors = False\n",
    "\n",
    "    # there seems to be an entry in fc_features with null SHAPE geometry.\n",
    "    # not sure how that happened, but this omits fc entries with null geometry\n",
    "    fc_features = fc_features[fc_features[\"SHAPE\"].notnull()].copy()\n",
    "\n",
    "    # convert  SHAPE column to shapely geometry\n",
    "    fc_features[\"geometry\"] = fc_features[\"SHAPE\"].apply(shape)\n",
    "    tread_features[\"geometry\"] = tread_features[\"SHAPE\"].apply(shape)\n",
    "    sidetrail_features[\"geometry\"] = sidetrail_features[\"SHAPE\"].apply(shape)\n",
    "\n",
    "    # convert points, AT treadway, side trails, and point features to GeoDataFrame\n",
    "    fc_gdf = gpd.GeoDataFrame(fc_features, geometry=\"geometry\", crs=3857)\n",
    "    tread_gdf = gpd.GeoDataFrame(tread_features, geometry=\"geometry\", crs=4269)\n",
    "    sidetrail_gdf = gpd.GeoDataFrame(sidetrail_features, geometry=\"geometry\", crs=4269)\n",
    "\n",
    "    # reproject line features to match point CRS\n",
    "    tread_projected = tread_gdf.to_crs(epsg=3857)\n",
    "    sidetrail_projected = sidetrail_gdf.to_crs(epsg=3857)\n",
    "\n",
    "    # process point features to gdf with appropriate CRS\n",
    "    pointfeature_gdfs = []\n",
    "    for fl in feature_layers:  # reuse the ones you already made earlier\n",
    "        sdf = fl.query(where=\"1=1\", out_fields=\"*\", return_geometry=True).sdf\n",
    "        sdf[\"geometry\"] = sdf[\"SHAPE\"].apply(shape)\n",
    "        gdf = gpd.GeoDataFrame(sdf, geometry=\"geometry\", crs=4269).to_crs(epsg=3857)\n",
    "        pointfeature_gdfs.append(gdf)\n",
    "\n",
    "    # perform unary union on line features (tread and side trails)\n",
    "    all_lines_gdf = pd.concat([tread_projected, sidetrail_projected], ignore_index=True)\n",
    "\n",
    "    # perform unary union on point features\n",
    "    all_points_gdf = pd.concat(pointfeature_gdfs, ignore_index=True)\n",
    "\n",
    "    # change buffer distance to m to match CRS\n",
    "    buffer_dist = BUFFER_FT * 0.3048\n",
    "\n",
    "    # create buffer from all features (line and point)\n",
    "    trail_buffer = all_lines_gdf.geometry.buffer(\n",
    "        buffer_dist\n",
    "    )  # THIS LINE TAKES A FEW MINUTES WHEN RUN INDEPENDENTLY\n",
    "    point_buffer = all_points_gdf.geometry.buffer(buffer_dist)\n",
    "\n",
    "    # combine buffers (unary union on each, and then union between both)\n",
    "    combined_buffer = trail_buffer.unary_union.union(point_buffer.unary_union)\n",
    "\n",
    "    # identify points outside the buffer\n",
    "    outside_points = fc_gdf[~fc_gdf.geometry.within(combined_buffer)].copy()\n",
    "\n",
    "    # log errors\n",
    "    summary_rows.append(\"Proximity errors:\")\n",
    "    if not outside_points.empty:\n",
    "        any_proximity_errors = True\n",
    "        outside_points.loc[:, \"error_type\"] = \"proximity\"\n",
    "        outside_points.loc[:, \"error_desc\"] = \"Point located beyond buffer zone\"\n",
    "        fc_error_rows.append(outside_points)\n",
    "        message = f\"{len(outside_points)} points found beyond buffer zone\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "        summary_rows.append(\"\")  # blank line between error checks\n",
    "    else:\n",
    "        message = f\"0 points found beyond buffer zone\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "        summary_rows.append(\"\")  # blank line between error checks\n",
    "\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error finding survey points beyond buffer zone\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\"    Proximity checks completed\\n\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# === collector-region check ===\n",
    "# function to compare attributes\n",
    "def match_club(row):\n",
    "    allowed_clubs = COLLECTOR_DICT.get(row['created_user'], [])\n",
    "    return row['Acronym'] in allowed_clubs\n",
    "\n",
    "# run collector-region check\n",
    "try:\n",
    "    # Nearest spatial join: attach nearest Acronym from tread_projected to each point\n",
    "    fc_with_club = gpd.sjoin_nearest(\n",
    "        fc_gdf,\n",
    "        tread_projected[['Acronym', 'geometry']],\n",
    "        how='left',\n",
    "        distance_col='distance_to_trail'\n",
    "    )\n",
    "\n",
    "    club_mismatch = fc_with_club[~fc_with_club.apply(match_club, axis=1)].copy()\n",
    "\n",
    "    # log errors\n",
    "    summary_rows.append(\"Collector-region errors:\")\n",
    "\n",
    "    if not club_mismatch.empty:\n",
    "        club_mismatch[\"error_type\"] = \"collector-region\"\n",
    "        club_mismatch[\"error_desc\"] = \"Collector not assigned to this trail region\"\n",
    "        fc_error_rows.append(club_mismatch)\n",
    "        message = f\"{len(club_mismatch)} features with mismatched collector-region assignment\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "        summary_rows.append(\"\")     # blank line between error checks\n",
    "    else:\n",
    "        message = f\"0 features with mismatched collector-region assignment\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "        summary_rows.append(\"\")     # blank line between error checks\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error finding collector-region mismatches\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\"    Collector-region check completed\\n\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# === FINAL EXPORTS ===\n",
    "# === create master XLSX of all issues ===\n",
    "try:\n",
    "    source = [\"summary\", \"fc\", \"rt\"]\n",
    "    error_list = [summary_rows, fc_error_rows, rt_error_rows]\n",
    "    error_list_order = [None, FC_ERROR_ORDER, RT_ERROR_ORDER]\n",
    "\n",
    "    # GlobalID and defGlobalID\n",
    "    cols_to_cap = [\"GlobalID\", \"defGlobalID\"]\n",
    "\n",
    "    output_name = f\"{CURRENT_DATE}_all_errors.xlsx\"\n",
    "    output_path = os.path.join(OUTPUT_FOLDER, output_name)\n",
    "\n",
    "    with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "\n",
    "        for source_name, error_rows, order in zip(source, error_list, error_list_order):\n",
    "            if error_rows:\n",
    "                if source_name == \"summary\":\n",
    "                    all_errors = pd.DataFrame(error_rows, columns=[\"Summary\"])\n",
    "                else:\n",
    "                    all_errors = pd.concat(error_rows, ignore_index=True)\n",
    "                    # sort columns for output\n",
    "                    all_errors = all_errors[order]\n",
    "                    # capitalize all letters in GlobalID and defGlobalID columns\n",
    "                    for col in cols_to_cap:\n",
    "                        if col in all_errors.columns:\n",
    "                            all_errors[col] = all_errors[col].astype(str).str.upper()\n",
    "\n",
    "                all_errors.to_excel(writer, sheet_name=source_name, index=False)\n",
    "                if source_name == \"summary\":\n",
    "                    logging.info(\n",
    "                        f\"    Exported error summary to sheet '{source_name}' in {output_path}\\n\"\n",
    "                    )\n",
    "                else:\n",
    "                    logging.info(\n",
    "                        f\"    Exported {len(all_errors)} errors to sheet '{source_name}' in {output_path}\\n\"\n",
    "                    )\n",
    "            else:\n",
    "                logging.info(f\"    No errors found in {source_name}!\\n\")\n",
    "except Exception as e:\n",
    "    logging.info(\"X   Error creating XLSX of errors\\n\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "\n",
    "\n",
    "# confirmation + log export message\n",
    "if WRITE_LOG:\n",
    "    logging.info(f\"Exported output summary to {FILE_PATH}\")\n",
    "    logging.info(\"All checks completed. Console outputs also saved to log file.\")\n",
    "else:\n",
    "    logging.info(\"All checks completed. (No log file written, console only)\")\n",
    "\n",
    "# calculate elapsed time\n",
    "end_time = datetime.datetime.now()\n",
    "elapsed_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "hours = int(elapsed_time_sec // 3600)\n",
    "minutes = int((elapsed_time_sec % 3600) // 60)\n",
    "seconds = int(elapsed_time_sec % 60)\n",
    "\n",
    "logging.info(f\"Elapsed Time: {hours:02d}:{minutes:02d}:{seconds:02d}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
